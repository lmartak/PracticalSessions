{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comprehensive_Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "iYx6oEVKcE_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise: putting everything together\n",
        "\n",
        "\n",
        "In this you will write code for a model that learns to classify mnist digits. You will use sonnet and tensorflow, tracking training progress with matplotlib."
      ]
    },
    {
      "metadata": {
        "id": "TGBJLkR_cI3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install dm-sonnet with pip. Include all necessary imports.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5gkBQpjJlCgP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nO_tMPdncmVy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fetch the mnist data from tf.keras.datasets.mnist.\n",
        "\n",
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Check what the data is like:\n",
        "print('Training dataset:')\n",
        "train_input, train_label = mnist_train\n",
        "print('* input shape:', train_input.shape)\n",
        "print('* input min, mean, max:', train_input.min(), train_input.mean(), train_input.max())\n",
        "print('* input dtype:', train_input.dtype)\n",
        "print('* label shape:', train_label.shape)\n",
        "print('* label min, mean, max:', train_label.min(), train_label.mean(), train_label.max())\n",
        "print('* label dtype:', train_label.dtype)\n",
        "\n",
        "test_input, test_label = mnist_test\n",
        "print('Number of test examples:', test_input.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utL4ZmLrepoH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalize the data into the \\[0, 1\\] interval. It's also a good idea to check the class distribution, but here we know that this is OK.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "60_4wXEPe7Ig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize both train_input and test_input so that it is in [0, 1].\n",
        "#\n",
        "# Also ensure the following data types:\n",
        "#\n",
        "# * train_input and test_input need to be np.float32.\n",
        "# * the labels need to be converted to np.int32.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDwRkDiYfzVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We can visualize the first few training examples using matplotlib.imshow()\n",
        "# in combination with the gallery function we defined.\n",
        "#\n",
        "# Copy the gallery function in this cell.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1WQD1huVgV8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show the first 6 training images on a 1x6 grid using the galerry function.\n",
        "# Remember to use grayscale plotting.\n",
        "# Also print their corresponding labels in the same order.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VZdwYo_fUpo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Write a function that turns the data into tensorflow datasets and into\n",
        "# tensors corresponding to batches of examples, returning these tensors.\n",
        "#\n",
        "# The train data should be\n",
        "#\n",
        "# * shuffled across the full dataset\n",
        "# * repeated indefinitely\n",
        "# * batched at size 64.\n",
        "#\n",
        "# Simply batch the test data.\n",
        "#\n",
        "# IMPORTANT: Add a final (singleton) axis to the inputs; the conv nets that\n",
        "# we will use will expect this.\n",
        "# * HINT: recall the `np.newaxis` from Intro_Numpy.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3JcANwNfHuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a sonnet module that has the following structure:\n",
        "#\n",
        "# 1. sonnet Conv2D with 16 channes, kernel shape 3, stride 1, padding 'SAME'\n",
        "# 2. max pooling with window_shape [3, 3], srides [2, 2], padding 'SAME'\n",
        "# 3. ReLU\n",
        "# 4. sonnet Conv2D with 16 channes, kernel shape 3, stride 1, padding 'SAME'\n",
        "# 5. Flatten the final conv features using snt.BatchFlatten\n",
        "# 5. A (dense) Linear layer with output_size = 10, the number of classes.\n",
        "#\n",
        "# You can write the sonnet module yourself, or use the helper module\n",
        "# snt.Sequential([..layers..to..connect..]).\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRp2hrGofH7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "(train_inputs, train_labels), (test_inputs, test_labels) = get_tf_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7daVkyoqS9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Instantiate a model\n",
        "# 2. Hook it up to the training data,\n",
        "# 3. Use the `tf.nn.sparse_softmax_cross_entropy_with_logits` op to define the loss\n",
        "# 4. Define the train_op that minimizes the loss (averaged over the batch)\n",
        "#    using the `GradientDescentOptimizer`. Set the learning rate to 0.01.\n",
        "# 5. Get the initialization op.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wvmlucn6vbSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Write a function that takes a list of losses and plots them.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tufk2Xa2qTEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run the training loop, keeping track of losses and potentially the accuracy\n",
        "# on the training set. Plot the loss curve intermittently.\n",
        "#\n",
        "# The simplest solution would add a new plot with each plotting call. You\n",
        "# can play with the frequency of plotting (and recording) a bit in order\n",
        "# to find something that works.\n",
        "#\n",
        "# Based on the loss curves, decide how to set your total number of training\n",
        "# iterations. Once you are satified, add some code that evaluates your\n",
        "# prediction accuracy (not loss!) on the test set.\n",
        "#\n",
        "# Note that the outputs from the network are logits; for prediction accuracy\n",
        "# we can pick the most likely label and see if it is correct.\n",
        "\n",
        "# The accuracy you should expect:\n",
        "#\n",
        "# * Roughly 90% after 1000 training steps.\n",
        "# * 97-98% after 10k training steps.\n",
        "#\n",
        "# First iterate with 1k steps, if that works, train for 10k. 10k steps will\n",
        "# be roughly 6 minutes on CPU.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fwSrI-c2Cn3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xvt4bOeP1Bbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChrJA2KOqTMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bonus: Solve the assignment using Keras\n",
        "\n",
        "* Build and train the model using [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) API\n",
        "* Refer to the [TensorFlow Keras documentation](https://www.tensorflow.org/guide/keras) but feel free to use official [Keras docs](https://keras.io/) for usage examples as well\n",
        "* Prefer [Functional API](https://www.tensorflow.org/guide/keras#functional_api) over [Sequential](https://www.tensorflow.org/guide/keras#sequential_model) one, as it is much more flexible in how the model architecture can be designed (Sequential API is meant for simple architectures comprising of single sequence of layers)\n",
        "* See [how `tf.keras` plugs with `tf.data`](https://www.tensorflow.org/guide/keras#input_tfdata_datasets)\n"
      ]
    },
    {
      "metadata": {
        "id": "XkNfYIRR6fnn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import (Input, Reshape, Conv2D, MaxPooling2D,\n",
        "                                     Flatten, Dense)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jujHYPhx7GiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}